{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNiFKskX4hNJ2ibRS2FqIkB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/evanphoward/cs242-final/blob/main/generate_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EzzlfI8RwHFE"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import sys\n",
        "import time\n",
        "import os\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.autograd import Variable\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Following Code loads the MNIST dataset"
      ],
      "metadata": {
        "id": "mp0KOg75CIn2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load datasets\n",
        "transform_train = transforms.Compose([                                   \n",
        "    transforms.RandomCrop(32, padding=4),                                       \n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307), (0.3081)),\n",
        "])\n",
        "trainset = torchvision.datasets.MNIST(root='./data', train=True, \n",
        "                                        download=True,\n",
        "                                        transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "# Load testing data\n",
        "transform_test = transforms.Compose([                                           \n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307), (0.3081)),\n",
        "])\n",
        "testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
        "                                       download=True,\n",
        "                                       transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=False,\n",
        "                                         num_workers=2)\n",
        "print('Finished loading datasets!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fw9zAdYL21fW",
        "outputId": "b6ce3721-7735-4fed-fa38-82f002ccaf49"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished loading datasets!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Following Code loads the CIFAR10 dataset"
      ],
      "metadata": {
        "id": "AIwWMymZCNzB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load datasets\n",
        "transform_train = transforms.Compose([                                   \n",
        "    transforms.RandomCrop(32, padding=4),                                       \n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)), (CIFAR10)\n",
        "])\n",
        "trainset = torchvision.datasets.MNIST(root='./data', train=True, \n",
        "                                        download=True,\n",
        "                                        transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "# Load testing data\n",
        "transform_test = transforms.Compose([                                           \n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)), (CIFAR10)\n",
        "])\n",
        "testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
        "                                       download=True,\n",
        "                                       transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=False,\n",
        "                                         num_workers=2)\n",
        "print('Finished loading datasets!')"
      ],
      "metadata": {
        "id": "mvV_G1vCCGDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sets up the CNN architecture. Make sure to flip the MNIST flag for CIFAR / MNIST training"
      ],
      "metadata": {
        "id": "WjSh_MKQCYwF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MNIST = True\n",
        "\n",
        "def conv_block(in_channels, out_channels, kernel_size=3, stride=1,\n",
        "               padding=1):\n",
        "    '''\n",
        "    A nn.Sequential layer executes its arguments in sequential order. In\n",
        "    this case, it performs Conv2d -> BatchNorm2d -> ReLU. This is a typical\n",
        "    block of layers used in Convolutional Neural Networks (CNNs). The \n",
        "    ConvNet implementation below stacks multiple instances of this three layer\n",
        "    pattern in order to achieve over 90% classification accuracy on CIFAR-10.\n",
        "    '''\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding,\n",
        "                  bias=False),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "class ConvNet(nn.Module):\n",
        "    '''\n",
        "    A 9 layer CNN using the conv_block function above. Again, we use a\n",
        "    nn.Sequential layer to build the entire model. The Conv2d layers get\n",
        "    progressively larger (more filters) as the model gets deeper. This \n",
        "    corresponds to spatial resolution getting smaller (via the stride=2 blocks),\n",
        "    going from 32x32 -> 16x16 -> 8x8. The nn.AdaptiveAvgPool2d layer at the end\n",
        "    of the model reduces the spatial resolution from 8x8 to 1x1 using a simple\n",
        "    average across all the pixels in each channel. This is then fed to the \n",
        "    single fully connected (linear) layer called classifier, which is the output\n",
        "    prediction of the model.\n",
        "    '''\n",
        "    def __init__(self):\n",
        "        super(ConvNet, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            conv_block(1 if MNIST else 3, 32),\n",
        "            conv_block(32, 32),\n",
        "            conv_block(32, 64, stride=2),\n",
        "            conv_block(64, 64),\n",
        "            conv_block(64, 64),\n",
        "            conv_block(64, 128, stride=2),\n",
        "            conv_block(128, 128),\n",
        "            conv_block(128, 256),\n",
        "            conv_block(256, 256),\n",
        "            nn.AdaptiveAvgPool2d(1)\n",
        "            )\n",
        "\n",
        "        self.classifier = nn.Linear(256, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        The forward function is called automatically by the model when it is\n",
        "        given an input image. It first applies the 8 convolution layers, then\n",
        "        finally the single classifier layer.\n",
        "        '''\n",
        "        h = self.model(x)\n",
        "        B, C, _, _ = h.shape\n",
        "        h = h.view(B, C)\n",
        "        return self.classifier(h)\n",
        "\n",
        "\n",
        "def train(net, epoch, train_loss_tracker, train_acc_tracker):\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        # update optimizer state\n",
        "        optimizer.step()\n",
        "        # compute average loss\n",
        "        train_loss += loss.item()\n",
        "        train_loss_tracker.append(loss.item())\n",
        "        loss = train_loss / (batch_idx + 1)\n",
        "        # compute accuracy\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "        acc = 100. * correct / total\n",
        "        # Print status\n",
        "        sys.stdout.write(f'\\rEpoch {epoch}: Train Loss: {loss:.3f}' +  \n",
        "                         f'| Train Acc: {acc:.3f}')\n",
        "        sys.stdout.flush()\n",
        "    train_acc_tracker.append(acc)\n",
        "    sys.stdout.flush()\n",
        "\n",
        "def test(net, epoch, test_loss_tracker, test_acc_tracker):\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            test_loss_tracker.append(loss.item())\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "            loss = test_loss / (batch_idx + 1)\n",
        "            acc = 100.* correct / total\n",
        "    sys.stdout.write(f' | Test Loss: {loss:.3f} | Test Acc: {acc:.3f}\\n')\n",
        "    sys.stdout.flush()\n",
        "    \n",
        "    # Save checkpoint.\n",
        "    acc = 100.*correct/total\n",
        "    test_acc_tracker.append(acc)"
      ],
      "metadata": {
        "id": "S6sA8pLr4jas"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda'\n",
        "net = ConvNet()\n",
        "net = net.to(device)\n",
        "epochs = 100\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(net.parameters(), lr=0.1, momentum=0.9,\n",
        "                            weight_decay=5e-4)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
        "\n",
        "# Records the training loss and training accuracy during training\n",
        "train_loss_tracker_ms, train_acc_tracker_ms = [], []\n",
        "\n",
        "# Records the test loss and test accuracy during training\n",
        "test_loss_tracker_ms, test_acc_tracker_ms = [], []\n",
        "\n",
        "start_time = time.time()\n",
        "for epoch in range(epochs):\n",
        "    train(net, epoch, train_loss_tracker_ms, train_acc_tracker_ms)\n",
        "    test(net, epoch, test_loss_tracker_ms, test_acc_tracker_ms)\n",
        "    scheduler.step()\n",
        "\n",
        "total_time = time.time() - start_time\n",
        "print('Total training time: {} seconds'.format(total_time))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_PCvdPmo8UbV",
        "outputId": "53d352a0-230e-4276-cbb4-737a7cfbbacd"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: Train Loss: 0.266| Train Acc: 91.200 | Test Loss: 0.292 | Test Acc: 90.660\n",
            "Epoch 1: Train Loss: 0.082| Train Acc: 97.478 | Test Loss: 0.129 | Test Acc: 95.930\n",
            "Epoch 2: Train Loss: 0.059| Train Acc: 98.145 | Test Loss: 0.082 | Test Acc: 97.420\n",
            "Epoch 3: Train Loss: 0.044| Train Acc: 98.687 | Test Loss: 0.040 | Test Acc: 98.610\n",
            "Epoch 4: Train Loss: 0.034| Train Acc: 98.995 | Test Loss: 0.035 | Test Acc: 98.860\n",
            "Total training time: 109.92180871963501 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(net.state_dict(), \"mnist_weights.pt\")"
      ],
      "metadata": {
        "id": "dfYSRG_MDW_U"
      },
      "execution_count": 16,
      "outputs": []
    }
  ]
}